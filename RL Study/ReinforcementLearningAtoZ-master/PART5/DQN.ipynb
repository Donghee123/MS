{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..') # add project root to the python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "\n",
    "from src.part3.MLP import MultiLayerPerceptron as MLP\n",
    "from src.part5.DQN import DQN, prepare_training_inputs\n",
    "from src.common.memory.memory import ReplayMemory\n",
    "from src.common.train_utils import to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-network (DQN)\n",
    "\n",
    "DQN의 전체적인 트레이닝 과정을 도식화 하면 다음과 같습니다.\n",
    "\n",
    "<img src=\"./images/dqn_overview.png\" width=\"40%\" height=\"40%\" title=\"DQN\" alt=\"DQN\"></img>\n",
    "\n",
    "DQN의 Loss 함수는 다음과 같습니다.\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\eta \\frac{\\partial \\frac{1}{m}\\sum_{i=1}^{m}\\mathcal{L}(s_i, a_i, r_i, s_i^{'})}{\\partial \\theta}$$\n",
    "\n",
    "$$ \\mathcal{L}(s_i, a_i, r_i, s_i^{'}) = |r_i+\\gamma \\max_{a'} Q_\\theta(s_i^{'},a')-Q_\\theta(s_i, a_i)|_2$$\n",
    "\n",
    "$$(s_i, a_i, r_i, s_i^{'}) \\sim \\mathcal{D}$$\n",
    "\n",
    "자 그럼 python 으로는 DQN을 어떻게 구현할 수 있을까요?\n",
    "```python\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 qnet: nn.Module,\n",
    "                 qnet_target: nn.Module,\n",
    "                 lr: float,\n",
    "                 gamma: float,\n",
    "                 epsilon: float):\n",
    "        \"\"\"\n",
    "        :param state_dim: input state dimension\n",
    "        :param action_dim: action dimension\n",
    "        :param qnet: main q network\n",
    "        :param qnet_target: target q network\n",
    "        :param lr: learning rate\n",
    "        :param gamma: discount factor of MDP\n",
    "        :param epsilon: E-greedy factor\n",
    "        \"\"\"\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.qnet = qnet\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.opt = torch.optim.Adam(params=self.qnet.parameters(), lr=lr)\n",
    "        self.register_buffer('epsilon', torch.ones(1) * epsilon)\n",
    "\n",
    "        # target network related\n",
    "        self.qnet_target = qnet_target\n",
    "        self.criteria = nn.SmoothL1Loss()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        qs = self.qnet(state)\n",
    "        prob = np.random.uniform(0.0, 1.0, 1)\n",
    "        if torch.from_numpy(prob).float() <= self.epsilon:  # random\n",
    "            action = np.random.choice(range(self.action_dim))\n",
    "        else:  # greedy\n",
    "            action = qs.argmax(dim=-1)\n",
    "        return int(action)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        s, a, r, ns = state, action, reward, next_state\n",
    "\n",
    "        # compute Q-Learning target with 'target network'\n",
    "        with torch.no_grad():\n",
    "            q_max, _ = self.qnet_target(ns).max(dim=-1, keepdims=True)\n",
    "            q_target = r + self.gamma * q_max * (1 - done)\n",
    "\n",
    "        q_val = self.qnet(s).gather(1, a)\n",
    "        loss = self.criteria(q_val, q_target)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.SmoothL1Loss()` 가 뭐지?\n",
    "\n",
    "Mean-squared Error (MSE) Loss 단점 중 하나는 데이터의 outlier에 매우 취약하다는 것입니다.\n",
    "모종의 이유로 타겟하는 레이블 y (우리의 경우는 td-target/q-learning target 이죠)이 noisy 할때를 가정하면, 잘못된 y 값을 맞추기 위해 파라미터들이 너무 sensitive 하게 움직이게 됩니다.\n",
    "\n",
    "이런 현상은 q-learning 의 학습초기에 매우 빈번해 나타날 것으로 예상할수 있습니다. 이러한 문제를 조금이라도 완화하기 위해서 outlier에 덜 민감한 loss 함수를 사용했습니다.\n",
    "\n",
    "### SmoothL1Loss (aka Huber loss)\n",
    "\n",
    "$$loss(x,y) = \\frac{1}{n}\\sum_i z_i$$\n",
    "$|x_i - y_i| <1$ 일때,\n",
    "$$z_i = 0.5(x_i - y_i)^2$$\n",
    "$|x_i - y_i| \\geq1$ 일때,\n",
    "$$z_i = |x_i - y_i|-0.5$$\n",
    "\n",
    "더욱 자세한 설명은 [여기](https://pytorch.org/docs/master/generated/torch.nn.SmoothL1Loss.html)를 참조해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 DQN논문에서 제안한 두 가지의 중요한 기법을 구현하는 방법에 대해서 알아봅시다. 그 두가지 기법은 다음과 같습니다.\n",
    "\n",
    "> 1. Target network 를 활용한 'Moving Target problem' 완화 <br>\n",
    "> 2. Sample 간의 시간적 연관관계를 줄이고, 한번에 더 많은 샘플을 학습할 수 있게 만든 Exeperience Replay "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target network 구현\n",
    "\n",
    "Target network는 main network와 동일한 구조 및 파라미터를 가지는 네트워트 였죠? `pytorch`에서는 과연 그럼 어떻게 target network를 구현할까요? <br>\n",
    "답은 간단합니다. Main network의 `state_dict`를 target network의 `state_dict`에 덮어쓰면 되겠죠?\n",
    "\n",
    "```python\n",
    "qnet_target.load_state_dict(qnet.state_dict())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay 구현\n",
    "\n",
    "Experience Replay는 간단하게 생각하면\n",
    "> (1) 기존의 transition sample 들을 저장하고 <br>\n",
    "> (2) 필요할 때, 저장된 샘플중에서 일부를 sampling 해서 돌려주는 장치입니다.\n",
    "\n",
    "이를 파이썬으로 구현하면 다음과 같이 구현할 수 있습니다.\n",
    "\n",
    "```python\n",
    "from random import sample\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, max_size):\n",
    "        # deque object that we've used for 'episodic_memory' is not suitable for random sampling\n",
    "        # here, we instead use a fix-size array to implement 'buffer'\n",
    "        self.buffer = [None] * max_size\n",
    "        self.max_size = max_size\n",
    "        self.index = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def push(self, obj):\n",
    "        self.buffer[self.index] = obj\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        self.index = (self.index + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = sample(range(self.size), batch_size)\n",
    "        return [self.buffer[index] for index in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 으로 `Cartpole-v1` 에서 학습하기\n",
    "\n",
    "앞에서 설명한 것들을 하나로 묶어서 DQN의 학습 과정을 구현해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter\n",
    "\n",
    "(심층) 강화학습 알고리즘에서 성능에 지대한 영향을 미치는 하이퍼파라미터들입니다.\n",
    "이 실습에 쓰인 하이퍼 파라미터는 https://github.com/seungeunrho/minimalRL/blob/master/dqn.py 에서 제안된 값들을 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4 * 5\n",
    "batch_size = 256\n",
    "gamma = 1.0\n",
    "memory_size = 50000\n",
    "total_eps = 3000\n",
    "eps_max = 0.08\n",
    "eps_min = 0.01\n",
    "sampling_only_until = 2000\n",
    "target_update_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joon0\\Anaconda3\\envs\\gpu_torch130\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "qnet = MLP(4, 2, num_neurons=[128])\n",
    "qnet_target = MLP(4, 2, num_neurons=[128])\n",
    "\n",
    "# initialize target network same as the main network.\n",
    "qnet_target.load_state_dict(qnet.state_dict())\n",
    "agent = DQN(4, 1, qnet=qnet, qnet_target=qnet_target, lr=lr, gamma=gamma, epsilon=1.0)\n",
    "env = gym.make('CartPole-v1')\n",
    "memory = ReplayMemory(memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    0 | Cumulative Reward :   16 | Epsilon : 0.080\n",
      "Episode :  100 | Cumulative Reward :   12 | Epsilon : 0.075\n",
      "Episode :  200 | Cumulative Reward :    9 | Epsilon : 0.070\n",
      "Episode :  300 | Cumulative Reward :   49 | Epsilon : 0.065\n",
      "Episode :  400 | Cumulative Reward :   66 | Epsilon : 0.060\n",
      "Episode :  500 | Cumulative Reward :   55 | Epsilon : 0.055\n",
      "Episode :  600 | Cumulative Reward :  275 | Epsilon : 0.050\n",
      "Episode :  700 | Cumulative Reward :  160 | Epsilon : 0.045\n",
      "Episode :  800 | Cumulative Reward :   53 | Epsilon : 0.040\n",
      "Episode :  900 | Cumulative Reward :   67 | Epsilon : 0.035\n",
      "Episode : 1000 | Cumulative Reward :  147 | Epsilon : 0.030\n",
      "Episode : 1100 | Cumulative Reward :   61 | Epsilon : 0.025\n",
      "Episode : 1200 | Cumulative Reward :   54 | Epsilon : 0.020\n",
      "Episode : 1300 | Cumulative Reward :  221 | Epsilon : 0.015\n",
      "Episode : 1400 | Cumulative Reward :   85 | Epsilon : 0.010\n",
      "Episode : 1500 | Cumulative Reward :   31 | Epsilon : 0.010\n",
      "Episode : 1600 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 1700 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 1800 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 1900 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 2000 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 2100 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 2200 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 2300 | Cumulative Reward :  222 | Epsilon : 0.010\n",
      "Episode : 2400 | Cumulative Reward :  326 | Epsilon : 0.010\n",
      "Episode : 2500 | Cumulative Reward :  500 | Epsilon : 0.010\n",
      "Episode : 2600 | Cumulative Reward :  486 | Epsilon : 0.010\n",
      "Episode : 2700 | Cumulative Reward :  277 | Epsilon : 0.010\n",
      "Episode : 2800 | Cumulative Reward :   11 | Epsilon : 0.010\n",
      "Episode : 2900 | Cumulative Reward :   13 | Epsilon : 0.010\n"
     ]
    }
   ],
   "source": [
    "print_every = 100\n",
    "\n",
    "for n_epi in range(total_eps):\n",
    "    # epsilon scheduling\n",
    "    # slowly decaying_epsilon\n",
    "    epsilon = max(eps_min, eps_max - eps_min * (n_epi / 200))\n",
    "    agent.epsilon = torch.tensor(epsilon)\n",
    "    s = env.reset()\n",
    "    cum_r = 0\n",
    "\n",
    "    while True:\n",
    "        s = to_tensor(s, size=(1, 4))\n",
    "        a = agent.get_action(s)\n",
    "        ns, r, done, info = env.step(a)\n",
    "\n",
    "        experience = (s,\n",
    "                      torch.tensor(a).view(1, 1),\n",
    "                      torch.tensor(r / 100.0).view(1, 1),\n",
    "                      torch.tensor(ns).view(1, 4),\n",
    "                      torch.tensor(done).view(1, 1))\n",
    "        memory.push(experience)\n",
    "\n",
    "        s = ns\n",
    "        cum_r += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if len(memory) >= sampling_only_until:\n",
    "        # train agent\n",
    "        sampled_exps = memory.sample(batch_size)\n",
    "        sampled_exps = prepare_training_inputs(sampled_exps)\n",
    "        agent.update(*sampled_exps)\n",
    "\n",
    "    if n_epi % target_update_interval == 0:\n",
    "        qnet_target.load_state_dict(qnet.state_dict())\n",
    "    \n",
    "    if n_epi % print_every == 0:\n",
    "        msg = (n_epi, cum_r, epsilon)\n",
    "        print(\"Episode : {:4.0f} | Cumulative Reward : {:4.0f} | Epsilon : {:.3f}\".format(*msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target update interval 에 따른 효과를 비교해봅시다.\n",
    "\n",
    "이전과 같이 wandb에서 결과를 확인해볼까요? 이 [링크](https://app.wandb.ai/junyoung-park/DQN?workspace=user-junyoung-park) 를 참조해주세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
